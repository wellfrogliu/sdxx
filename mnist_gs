mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
sess = tf.InteractiveSession()


def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, w):
    return tf.nn.conv2d(x, w, [1,1,1,1], padding='SAME')

def pool_max(x):
    return tf.nn.max_pool(x, [1,2,2,1], [1,2,2,1], padding='SAME')


x = tf.placeholder(tf.float32, [None, 784])
y_ = tf.placeholder(tf.float32, [None, 10])

x1 = tf.reshape(x, [-1, 28, 28, 1])
w1 = weight_variable([5,5,1,32])
b1 = bias_variable([32])
o1 = tf.nn.relu(conv2d(x1,w1) + b1)
p1 = pool_max(o1)

w2 = weight_variable([5,5,32,64])
b2 = bias_variable([64])
o2 = tf.nn.relu(conv2d(p1, w2) + b2)
p2 = pool_max(o2)

w3 = weight_variable([7*7*64, 1024])
b3 = bias_variable([1024])
x3 = tf.reshape(p2, [-1, 7*7*64])
o3 = tf.nn.relu(tf.matmul(x3, w3) + b3)
keep_prob = tf.placeholder(tf.float32)
d1 = tf.nn.dropout(o3, keep_prob)

w4 = weight_variable([1024,10])
b4 = bias_variable([10])
o4 = tf.nn.softmax(tf.matmul(d1, w4) + b4)

cross = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(o4), reduction_indices=[1]))
step = tf.train.AdamOptimizer(1e-4).minisize(cross)

acc = tf.equal(tf.argmax(y_,1),tf.argmax(o4,1))
acc_pre = tf.reduce_mean(tf.cast(acc, tf.float32))

tf.global_variables_initializer().run()
